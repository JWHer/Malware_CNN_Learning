import numpy as np
import random
import os
import keras
import time
import matplotlib.pyplot as plt
from keras.models import Sequential, load_model
from keras.layers import Dropout, Dense, Flatten
from keras.layers.convolutional import Conv2D, MaxPooling2D
from keras.callbacks import ModelCheckpoint
#from keras.preprocessing.image import ImageDataGenerator

class Loader:
    SIDE_X=21
    SIDE_Y=16
    SIZE=SIDE_X*SIDE_Y

    LOAD_SIZE=37*16

    def load(self, path, show=False):
        with open(path, 'r') as file:
            data=file.readline()
            data=data.split(',')[:self.LOAD_SIZE]
            data=[float(i) for i in data]
            data=np.array(data)
            data=data[256:]
            data=data.reshape(self.SIDE_X,self.SIDE_Y)

        if show:
            plt.figure(num=path)
            plt.title(path)
            plt.imshow(data, cmap='Greys')
            plt.show()

        return data.reshape(self.SIDE_X,self.SIDE_Y,1)

class saver:
    def writeFileByList(this, path, listArray):
        """path:save path   listArray:nd array"""
        if listArray is None:
            return

        # Open output file
        output = open(path, 'w', encoding='utf8')
        # Error
        if output is None:
            print("Invalid path")
            return 1

        # Write
        for i in range(0, len(listArray)):
            for j in range(0, len(listArray[0])):
                output.write(str(listArray[i][j]) + " ")
            output.write("\n")

        output.close
        return 0

def main():
    CLASS=2
    DROP=0.3
    VERBOSE=True#?
    LEARNING_RATE=0.01
    EPOCH=20
    DATA_PATH="C:\\Users\\hacel\\source\\repos\\VirusPretreatment\\VirusPretreatment\\6gram";
    #"C:\\Users\\hacel\\source\\repos\\VirusPretreatment\\VirusLearn\\data"
    LABEL_PATH="D:\\Train_ans.csv"
    #"C:\\Users\\hacel\\source\\repos\\VirusPretreatment\\VirusLearn\\trainLabels.csv"
    DATA_LOAD_NUMS=-1
    SAVE=True
    LOAD=False
    TEST=False
    MODEL_NAME="model.h5"

    # Data
    print("Getting dir infos...")
    files=[f for f in os.listdir(DATA_PATH) if f.rfind('.csv')!=-1]
    if DATA_LOAD_NUMS<0:
        DATA_LOAD_NUMS=len(files)
    random.shuffle(files)
    files=files[:DATA_LOAD_NUMS]
    files.sort()

    #if not TEST:
    print("Loading Labels...")
    y_csv=np.loadtxt(LABEL_PATH,
                        dtype={'names': ('Id', 'Class'),
                            'formats': ('|S32', np.int)},
                        delimiter=',')
    y_tmp=[d for d in y_csv if d[0].decode('utf-8') +'.csv' in files]
    y_tmp=sorted(y_tmp,key=lambda x:x[0])

    #y_tmp=[d[1] for d in y_csv if d[0].decode('utf-8') +'.csv' in files]
    y_data=[]
    for t in y_tmp:
        tmp=[0]*CLASS
        tmp[t[1]]=1
        y_data.append(tmp)
    y_data=np.array(y_data)

    print("Loading Datas...")
    loader=Loader()
    x_data=np.empty([len(files),loader.SIDE_X,loader.SIDE_Y,1])
    for i in range(len(files)):
        x_data[i]=loader.load(os.path.join(DATA_PATH, files[i]))

    #make test
    #if not TEST:
    x_test=x_data[-int(0.2*len(x_data)):]
    x_data=x_data[:int(0.8*len(x_data))]
    y_test=y_data[-int(0.2*len(y_data)):]
    y_data=y_data[:int(0.8*len(y_data))]

    start = time.time()

    if LOAD or TEST:
        #json_file=open("model.json", "r")
        #loaded_model_json=json_file.read()
        #json_file.close()
        #model=model_from_json(loaded_model_json)
        model=load_model(MODEL_NAME, custom_objects={
            'Adam':lambda **kwargs: hvd.DistributedOptimizer(keras.optimizers.Adam(**kwargs))})
    else:
        model = Sequential(name=MODEL_NAME)

        #model.add(Dense(loader.SIDE_X*loader.SIDE_Y, activation='relu', input_shape=(loader.SIDE_X*loader.SIDE_Y, )))
        #model.add(Dropout(rate=DROP))
        #model.add(Dense(loader.SIDE_X*loader.SIDE_Y, activation='relu') )
        #model.add(Dropout(rate=DROP))
        #model.add(Dense(2, activation='softmax'))

        # L1
        model.add( Conv2D(32, kernel_size=(3,3), strides=(1,1),
                         padding='same', activation='relu', input_shape=np.shape(x_data[0])) )
        model.add( MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same') )
        model.add( Dropout(rate=DROP))
        # L2
        model.add( Conv2D(64, kernel_size=(3,3), strides=(1,1),
                         padding='same', activation='relu') )
        model.add( MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same') )
        model.add( Dropout(rate=DROP))
        # Fully-Connected Layer
        model.add(Flatten())
        model.add(Dense(CLASS, activation='softmax', kernel_initializer='he_normal'))

        if VERBOSE: model.summary()
        Adam=keras.optimizers.Adam(lr=LEARNING_RATE, beta_1=0.9, beta_2=0.999, amsgrad=False)
        model.compile(optimizer=Adam, loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])

    if not TEST:
        history=model.fit(x_data, y_data, batch_size=100, epochs=EPOCH)
        score=model.evaluate(x_test, y_test)
        print('Accuracy:', score[1])
        print("경과시간 ", time.time()-start)

        # Show filter
        if False:
            # retrieve weights from the second hidden layer
            filters, biases = model.layers[0].get_weights()
            # normalize filter values to 0-1 so we can visualize them
            f_min, f_max = filters.min(), filters.max()
            filters = (filters - f_min) / (f_max - f_min)
            # plot first few filters
            n_filters, ix = 32, 1
            for i in range(n_filters):
	            # get the filter
	            f = filters[:, :, :, i]
	            # plot each channel separately
	            for j in range(1):
		            # specify subplot and turn of axis
		            ax = plt.subplot(n_filters, 1, ix)
		            ax.set_xticks([])
		            ax.set_yticks([])
		            # plot filter channel in grayscale
		            plt.imshow(f[:, :, j], cmap='gray')
		            ix += 1
            # show the figure
            plt.show()

        if True:
            from sklearn.metrics import roc_curve
            y_pred = model.predict(x_test).ravel()
            fpr, tpr, threshold = roc_curve(y_test.reshape(4000), y_pred)
            plt.plot(fpr, tpr)
            plt.xlabel('False positive rate')
            plt.ylabel('True positive rate')
            plt.title('ROC curve')
            plt.show()
            from sklearn.metrics import roc_auc_score
            auc = roc_auc_score(y_test.reshape(4000), y_pred)
            print(auc)

    if SAVE:
        #model_json=model.to_json()
        #with open("model.json", "w") as json_file:
        #    json_file.write(model_json)
        model.save(MODEL_NAME)
    
    if not TEST:
        x_data=np.concatenate((x_data, x_test))
        tp=[]
        tn=[]
        fp=[]
        fn=[]
        for i in range(len(x_data)):
            v=model.predict(np.array([x_data[i]]))
            if v[0,0]>v[0,1]:
                c=0
            else:
                c=1
            if y_tmp[i][1]==1 and c==1:
                tp.append([y_tmp[i][0].decode('utf-8'), v[0,1]])
            elif y_tmp[i][1]==1 and c==0:
                tn.append([y_tmp[i][0].decode('utf-8'), v[0,0]])
            elif y_tmp[i][1]==0 and c==1:
                fp.append([y_tmp[i][0].decode('utf-8'), v[0,1]])
            else:
                fn.append([y_tmp[i][0].decode('utf-8'), v[0,0]])

        tp=sorted(tp, key=lambda x: x[1], reverse=True)
        tn=sorted(tn, key=lambda x: x[1], reverse=True)
        fp=sorted(fp, key=lambda x: x[1], reverse=True)
        fn=sorted(fn, key=lambda x: x[1], reverse=True )
        sv=saver()
        sv.writeFileByList("out_tp.txt", tp)
        sv.writeFileByList("out_tn.txt", tn)
        sv.writeFileByList("out_fp.txt", fp)
        sv.writeFileByList("out_fn.txt", fn)
    else:
        #score=model.evaluate(x_test, y_test)
        #print('Accuracy:', score[1])
        out=[]
        for i in range(len(x_data)):
            v=model.predict(np.array([x_data[i]]))
            if v[0,0]>v[0,1]:
                # Benign ware
                c=0
            else:
                c=1
            out.append([files[i], c])
        sv=saver();
        sv.writeFileByList("result.txt", out)

if __name__=='__main__':
    main()
